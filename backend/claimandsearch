from openai import OpenAI
client = OpenAI()
import json


clusters = [['what is a woman and why is it important that we understand the difference between men and women?', "well, it's sort of easy to answer for me because a woman is somebody that can have a baby under certain circumstances.", 'a woman can, a woman has a quality.', "a woman is a person who's much smarter than a man i've always found.", "a woman is a person that doesn't give a man even a chance of success.", "and a woman's a person that in many cases has been treated very badly because i think that what happens with this crazy, this crazy issue of men being able to play in women's sports is just ridiculous and very unfair to women and very demeaning to women.", "women are basically incredible people, do so much for our country and we love, we love our women and we're going to take care of our women."],["Trump banned black people."]]

groupClusters = [""] * len(clusters)

text = ""

for i in range(len(clusters)):
    for j in range(len(clusters[i])):
        text = text + clusters[i][j] + " "
        groupClusters[i] += clusters[i][j] + " "
    groupClusters[i] = groupClusters[i][:-1]
text = text[:-1]


getClaimsTemplate = """
You are helping extract verifiable claims from social media videos.
I will give you the full transcipt for contextual purposes but just work with the sentences I want you to analyze.
These sentences may not be coherent together but they all follow a theme.

For the sentence(s) provided, do the following:
1. Identify a **main theme**.
2. Extract any **fact-checkable claims** -- things that can be verified through trusted sources. Rephrase them into a short unbiased searchable phrase with key words.
3. Ignore all vague, subjective, opinion-based, and non-factual content. It is okay if no claims are found.

Return the result in this JSON format:
{
    "theme": "Short theme",
    "claims": [
    {
        "id": 1,
        "text": claim 1
    } 
    {
        "id": 2,
        "text": claim 2
    }
    {
        etc...
    }
    ]
}

If there are no fact-checkable claims, return exactly this:
SKIP

The full transcript (to be used for context only) is as follow: %s
The sentence(s) that need to analyze are as follows: %s 
"""

JSONStr = "{\n"

for i in range(len(groupClusters)):

    JSONStr += "\"Topic #" + str(i + 1) + "\":\n"

    response = client.responses.create(
        model="gpt-4.1",
    
        input= getClaimsTemplate % (text, groupClusters[i])
    )
    JSONStr += response.output_text

    if i != len(groupClusters) - 1:
        JSONStr += ","
    JSONStr += "\n"

    
JSONStr += "}"


import requests
from dotenv import load_dotenv
import os
import json
from openai import OpenAI



load_dotenv()
googlecloud_api_key = os.getenv("GOOGLECLOUD_API_KEY")
search_engine_id = os.getenv("SEARCH_ENGINE_ID")

url = 'https://www.googleapis.com/customsearch/v1'



client = OpenAI()

analyzeEvidenceTemplate = """You are helping build a fact checking website.
You will be provided a claim along with google search results for that claim.
Your job is to decide whether the search results support or reject the claim.
The search results will be given in a json format as follows:
[
    {
    "Article #": 1
    "title": Title of article #1
    "link": Link to article #1
    "snippet": Snipped of article #1
    },
    {
    "Article #": 2
    "title": Title of article #2
    "link": Link to article #2
    "snippet": Snipped of article #2
    },
    {
    etc...
    }
]

Only base your answer on the information provided. Do not use any external or prior knowledge.
In your explanation do not reference that you have looked at articles just quote specific ones if needed.
Make it seem as though you have an answer and are not looking at snippets.
Return the result in this JSON format:
{
    "verdict": "True" | "False" | "Indecisive", // output indecisive if the claim is an opinion (ex: makes comments on what is fair or just) or there is not enough evidence.
    "explanation": "A brief explanation of your reasoning.",
    "links": [
        "https://example.com/article1",
        "https://example.com/article2"
    ] // Use links that support the verdict. Output no more than three links.
}

Here is the claim: %s

Here is the search: %s 
"""



data = json.loads(JSONStr)

for topic_key in data.keys():
    updatedClaims = []
    claim_id = 1
    for claim in data[topic_key]["claims"]:

        search_query = claim["text"]

        params = {
            'q': search_query,
            'key': googlecloud_api_key,
            'cx': search_engine_id
        }

        evidence = []

        response = requests.get(url, params=params)
        results = response.json().get("items", [])
        for i in range(len(results)):
            piece = {}
            piece["Article #"] = i + 1
            piece["title"] = results[i].get("title")
            piece["link"] = results[i].get("link")
            piece["snippet"] = results[i].get("snippet")
            evidence.append(piece)

        evStr = json.dumps(evidence, indent = 2)

        response = client.responses.create(
            model="gpt-4.1",
        
            input= analyzeEvidenceTemplate % (search_query, evStr)
        )

        evInterpJSON = response.output_text

        evInterp = json.loads(evInterpJSON)


        updatedClaim = {
            "id": claim["id"],
            "text": claim["text"],
            "verdict" : evInterp["verdict"],
            "explanation": evInterp["explanation"],
            "links": evInterp["links"]
        }

        updatedClaims.append(updatedClaim)

    data[topic_key]["claims"] = updatedClaims

print(json.dumps(data, indent = 4))



